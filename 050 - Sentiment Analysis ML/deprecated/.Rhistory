install.packages(c("ggplot2", "e1071", "caret", "quanteda", "irlba", "randomForest", "jsonlite", "RODBC", "wordcloud", "sentimentr", "magrittr", "dplyr", "pacman"))
library("RODBC")
library("devtools")
library("sentimentr")
library("magrittr")
options(scipen = 999)
syuzhet::afinn
syuzhet::afinn
syuzhet:::afinn
neg <- sum(syuzhet:::afinn[syuzhet:::afinn<0]);
neg <- sum(syuzhet:::afinn[syuzhet:::afinn$value<0]);
neg <- sum(syuzhet:::afinn$value[syuzhet:::afinn$value<0]);
neg <- sum(syuzhet:::afinn$value[syuzhet:::afinn$value<0]);
neg
sum(syuzhet:::afinn$value[syuzhet:::afinn$value<0]);
count(syuzhet:::afinn$value[syuzhet:::afinn$value<0]);
nrow(syuzhet:::afinn$value[syuzhet:::afinn$value<0]);
nrow(syuzhet:::afinn$value);
ncol(syuzhet:::afinn$value);
sum(syuzhet:::afinn$value[syuzhet:::afinn$value<0]);
length(syuzhet:::afinn$value[syuzhet:::afinn$value<0]);
length(syuzhet:::afinn$value[syuzhet:::afinn$value>0]);
lexicon
length(syuzhet:::syuzhet_dict$value[syuzhet:::afinn$value>0]);
lexicon
syuzhet:::syuzhet_dict
length(syuzhet:::syuzhet_dict$value[syuzhet:::syuzhet_dict$value>0]);
length(syuzhet:::syuzhet_dict$value[syuzhet:::syuzhet_dict$value<0]);
length(syuzhet:::syuzhet_dict$value[syuzhet:::afinn$value>0]);
length(syuzhet:::afinn$value[syuzhet:::afinn$value>0]);
length(syuzhet:::afinn$value[syuzhet:::afinn$value<0]);
lexicon:::hash_sentiment_asdf
lexicon::hash_sentiment_vadar
length(lexicon::hash_sentiment_vadar$y[lexicon::hash_sentiment_vadar$y<0]);
length(lexicon::hash_sentiment_vadar$y[lexicon::hash_sentiment_vadar$y>0]);
length(lexicon::hash_sentiment_vadar$y[lexicon::hash_sentiment_vadar$y=0]);
length(lexicon::hash_sentiment_vadar$y[lexicon::hash_sentiment_vadar$y==0]);
View(lexicon::hash_sentiment_vadar)
View(lexicon::emojis_sentiment)
lexicon::hash_sentiment_loughran_mcdonald
lexicon::hash_sentiment_loughran_mcdonald
length(lexicon::hash_sentiment_loughran_mcdonald$y[lexicon::hash_sentiment_loughran_mcdonald$y>0]);
length(lexicon::hash_sentiment_loughran_mcdonald$y[lexicon::hash_sentiment_loughran_mcdonald$y<00]);
length(lexicon::hash_sentiment_loughran_mcdonald$y[lexicon::hash_sentiment_loughran_mcdonald$y<0]);
length(lexicon::hash_sentiment_loughran_mcdonald$y[lexicon::hash_sentiment_loughran_mcdonald$y>0]);
length(lexicon::hash_sentiment_loughran_mcdonald$y[lexicon::hash_sentiment_loughran_mcdonald$y==0]);
lexicon::hash_sentiment_senticnet
length(lexicon::lexicon::hash_sentiment_senticnet$y[lexicon::lexicon::hash_sentiment_senticnet$y>0]);
length(lexicon::hash_sentiment_senticnet$y[lexicon::hash_sentiment_senticnet$y>0]);
length(lexicon::hash_sentiment_senticnet$y[lexicon::hash_sentiment_senticnet$y<0]);
length(lexicon::hash_sentiment_senticword$y[lexicon::hash_sentiment_sentiword$y<0]);
length(lexicon::hash_sentiment_sentiword$y[lexicon::hash_sentiment_sentiword$y>0]);
length(lexicon::hash_sentiment_sentiword$y[lexicon::hash_sentiment_sentiword$y<00]);
length(lexicon::hash_sentiment_sentiword$y[lexicon::hash_sentiment_sentiword$y<0]);
View(lexicon::hash_sentiment_vadar)
View(lexicon::hash_sentiment_affin)
View(lexicon::afinn)
syuzhet:::afinn
install.packages(c("ggplot2", "RODBC","devtools", "RTextTools", "caret"))
library("RODBC")
library("devtools")
library("RTextTools")
library("e1071")
library("caret")
options(scipen = 999)
setwd("C:/Users/hupse/OneDrive/R/05 - ML RTextTools")
conn <- odbcDriverConnect('driver={SQL Server};server=localhost\\SQLEXPRESS;database=iPhoneXReleaseTweets;trusted_connection=true')
positive_all = sqlQuery(conn, "SELECT TOP 1500 [tweet]
FROM dbo.CrowdFlower AS TESTDATA
WHERE (relevant_yn LIKE 'yes') AND (relevant_yn_confidence = 1) AND (sentiment_confidence > 0.6) AND (sentiment LIKE 'positive')
ORDER BY NEWID()"
, as.is = TRUE)
positive = positive_all[1:1400,]
positive_test = positive_all[1401:1500,]
negative_all = sqlQuery(conn, "SELECT TOP 1500 [tweet]
FROM dbo.CrowdFlower AS TESTDATA
WHERE (relevant_yn LIKE 'yes') AND (relevant_yn_confidence = 1) AND (sentiment_confidence > 0.6) AND (sentiment LIKE 'negative')
ORDER BY NEWID()"
, as.is = TRUE)
negative = negative_all[1:1400,]
negative_test = negative_all[1401:1500,]
tweet = c(positive, negative)
tweet_test= c(positive_test, negative_test)
tweet_all = c(tweet, tweet_test)
sentiment = c(rep("positive", length(positive) ),
rep("negative", length(negative)))
sentiment_test = c(rep("positive", length(positive_test) ),
rep("negative", length(negative_test)))
sentiment_all = as.factor(c(sentiment, sentiment_test))
mat= create_matrix(tweet_all, language="english",
removeStopwords=TRUE, removeNumbers=TRUE, removePunctuation=TRUE, toLower=TRUE, removeSparseTerms=0.999,
stemWords=TRUE, tm::weightTf)
container = create_container(mat, as.numeric(sentiment_all),
trainSize=1:2800, testSize=2801:3000,virgin=FALSE)
models = train_models(container, algorithms=c("MAXENT",
"SVM"))
#"GLMNET", "BOOSTING",
#"SLDA","BAGGING",
#"RF", # "NNET",
#"TREE"))
# test the model
results = classify_models(container, models)
table(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"SVM_LABEL"])
recall_accuracy(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"SVM_LABEL"])
confusionMatrix(table(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"SVM_LABEL"]))
analytics = create_analytics(container, results)
summary(analytics)
head(analytics@algorithm_summary)
head(analytics@label_summary)
head(analytics@document_summary)
analytics@ensemble_summary
N=10
cross_SVM = cross_validate(container,N,"SVM")
cross_GLMNET = cross_validate(container,N,"GLMNET")
cross_MAXENT = cross_validate(container,N,"MAXENT")
# Cross Validation
N=10
cross_SVM = cross_validate(container,N,"SVM")
cross_MAXENT = cross_validate(container,N,"MAXENT")
results = classify_models(container, models)
table(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"SVM_LABEL"])
recall_accuracy(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"SVM_LABEL"])
confusionMatrix(table(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"SVM_LABEL"]))
analytics = create_analytics(container, results)
summary(analytics)
head(analytics@algorithm_summary)
head(analytics@label_summary)
head(analytics@document_summary)
analytics@ensemble_summary
table(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"MAXENT_LABEL"])
recall_accuracy(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"MAXENT_LABEL"])
confusionMatrix(table(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"MAXENT_LABEL"]))
results
table(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"MAXENTROPY_LABEL"])
recall_accuracy(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"MAXENTROPY_LABEL"])
confusionMatrix(table(as.numeric(as.numeric(sentiment_all[2801:3000])), results[,"MAXENTROPY_LABEL"]))
models = train_models(container, algorithms=c("MAXENT",
"SVM",
"GLMNET", "BOOSTING",
"SLDA","BAGGING",
"RF",  "NNET",
"TREE"))
mat= create_matrix(tweet_all, language="english",
removeStopwords=TRUE, removeNumbers=TRUE, removePunctuation=TRUE, toLower=TRUE, removeSparseTerms=0.999,
stemWords=TRUE, tm::weightTf)
container = create_container(mat, as.numeric(sentiment_all),
trainSize=1:2800, testSize=2801:3000,virgin=FALSE)
models = train_models(container, algorithms=c("MAXENT",
"SVM",
"GLMNET", "BOOSTING",
"SLDA","BAGGING",
"RF",  "NNET",
"TREE"))
?train_models
?algorithms
